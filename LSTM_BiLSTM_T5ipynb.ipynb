{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnhidbd8wWyq",
        "outputId": "01dbc6d8-b85f-4eb1-eb99-d6f2143174ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import json\n",
        "\n",
        "# Load the data\n",
        "with open('train.jsonl', 'r', encoding='utf-8') as file:\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "# Set up T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate spoilers using T5 model\n",
        "def generate_spoiler(prompt):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    output = model.generate(input_ids, max_length=150, num_beams=2, length_penalty=0.8, early_stopping=True)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate spoilers and print the results\n",
        "for entry in data:\n",
        "    uuid = entry['uuid']\n",
        "    clickbait_post = entry['postText'][0]\n",
        "    linked_document = '\\n'.join(entry['targetParagraphs'])\n",
        "    prompt = f\"Clickbait post: {clickbait_post}\\nLinked document: {linked_document}\"\n",
        "    spoiler = generate_spoiler(prompt)\n",
        "\n",
        "    output = {\n",
        "        \"uuid\": uuid,\n",
        "        \"spoiler\": spoiler\n",
        "    }\n",
        "\n",
        "    print(json.dumps(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53b5J0yrkkkx",
        "outputId": "9deaceb0-c0f2-44fc-e45c-28fd994e35c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_file_path = '/content/drive/MyDrive/train.jsonl'\n",
        "val_file_path = '/content/drive/MyDrive/train.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti56VEss1Bxl",
        "outputId": "2ed4a5de-3246-4441-aaa5-512a139c9f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       Wes Welker Wanted Dinner With Tom Brady, But P...\n",
            "1       NASA sets date for full recovery of ozone hole...\n",
            "2       This is what makes employees happy -- and it's...\n",
            "3       Passion is overrated — 7 work habits you need ...\n",
            "4       The perfect way to cook rice so that it's perf...\n",
            "                              ...                        \n",
            "3195    Has Facebook's video explosion completely shak...\n",
            "3196    Cop Is Eating At A Chili's When Teen Hands Him...\n",
            "3197    5 popular myths about visible signs of aging t...\n",
            "3198    You need to see this Twitter account that pred...\n",
            "3199    GOP congressman comes out for gay marriage Pen...\n",
            "Name: text, Length: 3200, dtype: object\n",
            "0       passage\n",
            "1        phrase\n",
            "2        phrase\n",
            "3         multi\n",
            "4        phrase\n",
            "         ...   \n",
            "3195    passage\n",
            "3196    passage\n",
            "3197      multi\n",
            "3198     phrase\n",
            "3199     phrase\n",
            "Name: tags, Length: 3200, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open(train_file_path, 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Combine 'postText' and 'targetTitle'\n",
        "df['text'] = df['postText'].apply(lambda x: ' '.join(x)) + ' ' + df['targetTitle']\n",
        "\n",
        "df['tags'] = df['tags'].apply(lambda x: ','.join(map(str, x)))\n",
        "\n",
        "\n",
        "# Extracted data\n",
        "texts = df['text']\n",
        "labels = df['tags']\n",
        "\n",
        "print(texts)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGUeogmGxfs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGDHK-f0eNya",
        "outputId": "924a17c6-e7e0-4b91-d7a5-e413c31ccd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 2 ... 0 2 2]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "print(encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ludgHLaTfGnP",
        "outputId": "00921654-0212-40b2-9b84-aca8faad89da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming 'encoded_labels' have integer-encoded labels (0, 1, 2)\n",
        "one_hot_labels = to_categorical(encoded_labels, num_classes=3)\n",
        "print(one_hot_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgE0e3uCGIMW",
        "outputId": "c0690ca5-a094-486a-af49-1b070230895d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "80/80 [==============================] - 7s 60ms/step - loss: 1.0352 - accuracy: 0.4266 - val_loss: 1.0142 - val_accuracy: 0.4922\n",
            "Epoch 2/8\n",
            "80/80 [==============================] - 3s 34ms/step - loss: 0.7595 - accuracy: 0.7086 - val_loss: 0.9757 - val_accuracy: 0.5594\n",
            "Epoch 3/8\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.2888 - accuracy: 0.9066 - val_loss: 1.2006 - val_accuracy: 0.5328\n",
            "Epoch 4/8\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0793 - accuracy: 0.9781 - val_loss: 1.6336 - val_accuracy: 0.5437\n",
            "Epoch 5/8\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.0517 - accuracy: 0.9863 - val_loss: 1.7415 - val_accuracy: 0.5656\n",
            "Epoch 6/8\n",
            "80/80 [==============================] - 1s 12ms/step - loss: 0.0286 - accuracy: 0.9937 - val_loss: 2.2543 - val_accuracy: 0.5406\n",
            "Epoch 7/8\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0541 - accuracy: 0.9859 - val_loss: 1.9033 - val_accuracy: 0.5391\n",
            "Epoch 8/8\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 1.6344 - val_accuracy: 0.5250\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79e84c11f460>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# tokenizer.fit_on_texts(texts_train)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_input = pad_sequences(input_sequences)\n",
        "\n",
        "# tags = labels_train\n",
        "tags = one_hot_labels\n",
        "\n",
        "# learning_rate = 0.01\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, output_dim=100, input_length=padded_input.shape[1]))\n",
        "model.add(LSTM(100)) # 100 neurons\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #Default learning rate is 0.001\n",
        "\n",
        "\n",
        "# Compile the model with the optimizer, learning rate, and loss function\n",
        "# optimizer = Adam(learning_rate=learning_rate)\n",
        "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_input, tags, epochs=8, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq5kBy38FMvr",
        "outputId": "23b1a296-6852-42c2-af4c-24d500d31ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "80/80 [==============================] - 10s 94ms/step - loss: 1.0371 - accuracy: 0.4402 - val_loss: 1.0159 - val_accuracy: 0.5219\n",
            "Epoch 2/8\n",
            "80/80 [==============================] - 3s 37ms/step - loss: 0.7589 - accuracy: 0.6891 - val_loss: 0.9377 - val_accuracy: 0.5797\n",
            "Epoch 3/8\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.2639 - accuracy: 0.9137 - val_loss: 1.2104 - val_accuracy: 0.5266\n",
            "Epoch 4/8\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0976 - accuracy: 0.9746 - val_loss: 1.6476 - val_accuracy: 0.5391\n",
            "Epoch 5/8\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0606 - accuracy: 0.9871 - val_loss: 1.6591 - val_accuracy: 0.5281\n",
            "Epoch 6/8\n",
            "80/80 [==============================] - 1s 10ms/step - loss: 0.0409 - accuracy: 0.9906 - val_loss: 2.0019 - val_accuracy: 0.5594\n",
            "Epoch 7/8\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0220 - accuracy: 0.9949 - val_loss: 1.7629 - val_accuracy: 0.5562\n",
            "Epoch 8/8\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0183 - accuracy: 0.9953 - val_loss: 1.9819 - val_accuracy: 0.5469\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79e863953b20>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_input = pad_sequences(input_sequences)\n",
        "\n",
        "# tags = labels_train\n",
        "tags = one_hot_labels\n",
        "\n",
        "# Build BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, output_dim=100, input_length=padded_input.shape[1]))\n",
        "model.add(Bidirectional(LSTM(100)))  # BiLSTM layer with 100 neurons\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_input, tags, epochs=8, validation_split=0.2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
